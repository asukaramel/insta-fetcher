import requests
import os
import csv
import datetime
import urllib.request
import gspread
import re
import logging
from oauth2client.service_account import ServiceAccountCredentials
import schedule
import time
from datetime import datetime as dt
import pytz

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
if not ACCESS_TOKEN:
    raise Exception("ç’°å¢ƒå¤‰æ•° 'ACCESS_TOKEN' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# ====== è¨­å®šé …ç›® ======
HASHTAG = 'å¾³å·åœ’'
INSTAGRAM_BUSINESS_ID = '17841413261363491'
ACCESS_TOKEN = 'EAAH9Mdud0HQBOZCXyCReEgIcpZAHTijRi4GGWyWZAKNBofp7kxjm3L6b4ZCWbFEbTXEHkd7RvnWYzIROHOzhyk5nOXyorNcmm6vjNHJapx1XibLNkW2uJJ9ZCgrysb0JkGM76V5V1UzOWKcmOW5UXU9UdroO8Sz9EgDKZCLELUjpIwjHXGvqvwzBZCwBLPNatT4XrZBti4Ai'
SAVE_DIR = '/Users/asuka_2752/Documents/INSTA/Images'  # ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
CSV_PATH = '/Users/asuka_2752/Documents/INSTA/log.csv'
SPREADSHEET_NAME = 'InstaContestLog'
GOOGLE_CREDENTIALS_PATH = 'client_secret.json'  # èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
SLACK_WEBHOOK_URL = 'https://hooks.slack.com/services/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'  # â† ã‚ãªãŸã®Slack Webhook URLã«ã™ã‚‹
# =======================

# âœ… ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    filename='log.txt',
    filemode='a',
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
)

def log(message):
    print(message)
    logging.info(message)

# âœ… Slacké€šçŸ¥
def notify_slack(message):
    payload = {'text': message}
    try:
        response = requests.post(SLACK_WEBHOOK_URL, json=payload)
        if response.status_code != 200:
            log(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {response.status_code} {response.text}")
    except Exception as e:
        log(f"Slacké€šçŸ¥ä¾‹å¤–: {e}")

# ï¼ˆä»¥é™ã¯åŒã˜ï¼‰
# Google Sheets èªè¨¼
def get_gspread_client():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('client_secret.json', scope)
    return gspread.authorize(creds)

# Instagram APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
def instagram_api(url):
    headers = {'Authorization': f'Bearer {ACCESS_TOKEN}'}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as e:
        log(f"HTTP Error {response.status_code} : {response.text}")
        raise
    except Exception as e:
        log(f"Unexpected error: {e}")
        raise

# ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°IDã‚’å–å¾—
def get_hashtag_id():
    url = f"https://graph.facebook.com/v23.0/ig_hashtag_search?user_id={INSTAGRAM_BUSINESS_ID}&q={HASHTAG}&access_token={ACCESS_TOKEN}"
    data = instagram_api(url)
    hashtag_id = data['data'][0]['id']
    log(f"Hashtag ID for '{HASHTAG}' is {hashtag_id}")
    return hashtag_id


# æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ä»¶å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
def fetch_posts():
    hashtag_id = get_hashtag_id()
    url = (
        f"https://graph.facebook.com/v23.0/{hashtag_id}/recent_media"
        f"?user_id={INSTAGRAM_BUSINESS_ID}&fields=id,timestamp,media_url,like_count,comments_count,permalink,caption"
        f"&limit=50&access_token={ACCESS_TOKEN}"
    )
    posts = []
    while url:
        data = instagram_api(url)
        posts.extend(data['data'])
        url = data.get('paging', {}).get('next')
        time.sleep(1)  # â† ã“ã“ã§1ç§’å¾…ã¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ç§’æ•°èª¿æ•´ï¼‰
    return posts

# ç”»åƒã‚’ä¿å­˜
def download_image(url, path):
    urllib.request.urlretrieve(url, path)

# CSV ã«è¨˜éŒ²æ¸ˆã¿ ID ã‚’ãƒ­ãƒ¼ãƒ‰
def load_existing_ids():
    if not os.path.exists(CSV_PATH):
        return set()
    with open(CSV_PATH, newline='', encoding='utf-8') as f:
        return set(row[1] for row in csv.reader(f))[1:]

# CSV ã«ä¿å­˜
def save_to_csv(post, file_name, timestamp_str):
    is_new = not os.path.exists(CSV_PATH)
    with open(CSV_PATH, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if is_new:
            writer.writerow(['timestamp', 'id', 'filename', 'like_count', 'comment_count', 'caption', 'permalink', 'media_url'])
        writer.writerow([
            timestamp_str,
            post['id'],
            file_name,
            post.get('like_count', 0),
            post.get('comments_count', 0),
            post.get('caption', ''),
            post['permalink'],
            post['media_url'],
        ])

# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
def save_to_gsheet(post, file_name, timestamp_str, sheet):
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒãªã‘ã‚Œã°è¿½åŠ 
    if sheet.row_count == 0 or sheet.cell(1, 1).value is None:
        sheet.append_row(['timestamp', 'id', 'filename', 'like_count', 'comment_count', 'caption', 'permalink', 'media_url'])

    # â€» existing_ids_gsheet ã®ãƒã‚§ãƒƒã‚¯ã¯ job() å´ã§ã‚„ã£ã¦ã„ã‚‹ã®ã§ã“ã“ã¯ä¸è¦ï¼
    sheet.append_row([
        timestamp_str,
        post['id'],
        file_name,
        post.get('like_count', 0),
        post.get('comments_count', 0),
        post.get('caption', ''),
        post['permalink'],
        post['media_url'],
    ])
    log(f"â†’ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜: {post['id']}")


# ç”»åƒé€£ç•ªç”¨ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·å–å¾—
def get_next_file_number():
    os.makedirs(SAVE_DIR, exist_ok=True)
    existing_files = [f for f in os.listdir(SAVE_DIR) if re.match(r'tokugawa_(\d+)\.jpeg', f)]
    if not existing_files:
        return 1
    numbers = [int(re.match(r'tokugawa_(\d+)\.jpeg', f).group(1)) for f in existing_files]
    return max(numbers) + 1

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def job():
    log("===== ã‚¸ãƒ§ãƒ–é–‹å§‹ =====")
    os.makedirs(SAVE_DIR, exist_ok=True)
    existing_ids = load_existing_ids()
    posts = fetch_posts()
    gc = get_gspread_client()
    sheet = gc.open(SPREADSHEET_NAME).sheet1

    # âœ… Google Sheetså´ã®æ—¢å­˜IDã‚’1å›ã ã‘å–å¾—
    existing_ids_gsheet = sheet.col_values(2)

    file_counter = get_next_file_number()
    jst = pytz.timezone('Asia/Tokyo')

    new_count = 0

    for post in posts:
        # CSVå´ã§æ—¢ã«å–å¾—æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if post['id'] in existing_ids:
            continue
        # Google Sheetså´ã§ã‚‚æ—¢ã«å–å¾—æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if post['id'] in existing_ids_gsheet:
            continue

        # timestampãƒ‘ãƒ¼ã‚¹å¯¾å¿œï¼ˆ+0000å¯¾å¿œï¼‰
        timestamp_utc = dt.strptime(post['timestamp'], '%Y-%m-%dT%H:%M:%S%z')
        timestamp_jst = timestamp_utc.astimezone(jst)
        timestamp_str = timestamp_jst.strftime('%Y-%m-%d %H:%M:%S')

        file_name = f'tokugawa_{file_counter}.jpeg'
        file_counter += 1

        image_path = os.path.join(SAVE_DIR, file_name)
        download_image(post['media_url'], image_path)

        save_to_csv(post, file_name, timestamp_str)
        save_to_gsheet(post, file_name, timestamp_str, sheet)

        log(f"[NEW] {post['id']} â†’ {file_name}")
        new_count += 1

    log(f"===== ã‚¸ãƒ§ãƒ–çµ‚äº†: æ–°è¦å–å¾— {new_count} ä»¶ =====")

    # âœ… Slacké€šçŸ¥
    notify_slack(f"âœ… Instagram Fetcher å®Œäº†ï¼ æ–°è¦å–å¾— {new_count} ä»¶ ( {dt.now().strftime('%Y-%m-%d %H:%M:%S')} )")


 


# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œ
schedule.every(6).hours.do(job)

if __name__ == "__main__":
    log("Instagram Fetcher started. Press Ctrl+C to stop.")
    notify_slack("ğŸš€ Instagram Fetcher èµ·å‹•ã—ã¾ã—ãŸï¼")
    job()  # æœ€åˆã«ä¸€åº¦å®Ÿè¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)